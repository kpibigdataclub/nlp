{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_data(data, target_names):\n",
    "    try:\n",
    "        if (len(data) != 3578) or (len(target_names) != 4) : print 'Incorrect data!'\n",
    "        else: print \"Well done!\"\n",
    "    except: print \"Something is wrong...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic clustering with sci-kit learn\n",
    "## 20 newsgroups dataset\n",
    "### For topic clustering we will use the 20 newsgroups dataset. This dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training and the other one for testing. The split between the train and test set is based upon messages posted before and after a specific date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also we will use sci-kit learn library.\n",
    "### You can start by importing some stuff that we`ll use during our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# categories, that we will try to cluster\n",
    "categories = [\n",
    "    'comp.windows.x',\n",
    "    'talk.religion.misc',\n",
    "    'misc.forsale',\n",
    "    'sci.space']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Let`s import dataset!\n",
    "### Tip: don`t forget to shuffle data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(<FILL_IN>, random_state=42)\n",
    "\n",
    "print \"%d documents\" % len(dataset.data)\n",
    "print \"%d categories\" % len(dataset.target_names)\n",
    "check_data(dataset.data, dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = dataset.target # categories for each sample, list\n",
    "true_k = np.unique(labels).shape[0] # category count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF\n",
    "### Everything is ready to tf-idf transformation. We will use TfidfVectorizer to do it.\n",
    "### Your task is to choose wright parameters. Remember, that you also need to remove stop-words for better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extracting features from the training dataset using a sparse vectorizer\"\n",
    "t0 = time()\n",
    "\n",
    "# Maximum number of features (dimensions) to extract from text.\n",
    "n_features = 10000\n",
    "\n",
    "# vectorization \n",
    "vectorizer = TfidfVectorizer(<FILL_IN>)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print \"\\nDone in %fs\" % (time() - t0)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering\n",
    "### Finaly, we will make clustering using k-means, so fill in all gaps and just do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km = KMeans(<FILL_IN>)\n",
    "\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here you can see top words for each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print \"Cluster %d:\" % i, \n",
    "    for ind in order_centroids[i, :15]:\n",
    "        print ' %s' % terms[ind], \n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
